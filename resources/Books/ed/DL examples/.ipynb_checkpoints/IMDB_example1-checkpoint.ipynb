{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FuwspVjohEPn",
   "metadata": {
    "id": "FuwspVjohEPn"
   },
   "source": [
    "> There are 50,000 reviews, 25,000 for training and 25,000 for testing\n",
    "> Each set consisting of 50% positive and 50% negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ad385e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6ad385e",
    "outputId": "cabe7e3c-5315-426c-d946-0fc566dc051d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-515fc6d3d998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# LOAD DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# num_words=10000 will keep top 10,000 most frequently occuring words in the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "# num_words=10000 will keep top 10,000 most frequently occuring words in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tcjT5H4fiOCT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcjT5H4fiOCT",
    "outputId": "88390406-d236-49f7-9009-5356f14b5c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# As we mentioned data is splitted into 25,000 for training and 25,000 for testing\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "564505b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "564505b4",
    "outputId": "d3cf21f2-fb36-4248-f399-532b6a3e6d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data and test_data are list of reviews, each review is a list of word indices\n",
    "# train_lables and test_labels are list of 0's and 1's, where 0 is negative and 1 is positive\n",
    "\n",
    "len(train_data[0]) # there are 218 words in 1st review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a89c940",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a89c940",
    "outputId": "85598396-c9f5-4aa8-a4a2-3e6084441481"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0] # 1st review is positive review, 0 is negative and 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1940f0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1940f0e",
    "outputId": "e5a23c9f-d6df-4644-87fb-9e441e2440ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:\n",
    "\n",
    "# train_data mai 25,000 reviews hain hum har review ka max lenge or uski list banayegen to 25,000 reviews ka \n",
    "# ..1 maximum hoga to 25,000 ki list banjaygi then hum us list ka max lenge to 9999 ayega because we are taking top 10,000 words\n",
    "\n",
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca252bf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca252bf1",
    "outputId": "043e4f2c-156e-4e7a-d1fc-7fe16dea0009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1113\n"
     ]
    }
   ],
   "source": [
    "# Decode review back to English\n",
    "\n",
    "word_index = imdb.get_word_index() # it is a dict mapping words to an integer index\n",
    "\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()] # Reverses it, mapping integer indices to words\n",
    ")\n",
    "\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i - 3, '?') for i in train_data[0]] # decodes the review\n",
    ")\n",
    "\n",
    "# indices are offset by 3 because 0, 1, and 2 are reserved indices for \"padding\", \"start of sequence\" and \"unknown\"\n",
    "# word_index # 'fawn': 34701,\n",
    "# reverse_word_index # 34701: 'fawn',\n",
    "##############\n",
    "decoded_review1 = []\n",
    "' '.join(\n",
    "    for i in train_data[0]:\n",
    "        decoded_review1.append(reverse_word_index.get(i, '?'))\n",
    "        print(decoded_review1)\n",
    "        \n",
    ")\n",
    "\n",
    "print(type(decoded_review))\n",
    "print(len(decoded_review))\n",
    "# print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d169a4",
   "metadata": {
    "id": "56d169a4"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb4246bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb4246bf",
    "outputId": "c261679d-40b7-40ba-9713-66e8ab506cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Encoding the integer sequences into a binary matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def vectorize_sequences(sequences, dimension=10000):\n",
    "#     results = np.zeros((len(sequences), dimension)) # creates all_zero_matrix of shape (len(sequences), dimension)\n",
    "#     for i, sequence in enumerate(sequences):\n",
    "#         results[i, sequence] = 1 # sets specific indices of results[i] to 1s\n",
    "#     return results\n",
    "\n",
    "# x_train = vectorize_sequences(train_data)\n",
    "# x_test = vectorize_sequences(test_data)\n",
    "\n",
    "results = np.zeros((len(train_data), 10000))\n",
    "\n",
    "print(results)\n",
    "print(len(train_data))\n",
    "\n",
    "# for i, sequence in enumerate(train_data):\n",
    "#   print(i, sequence)\n",
    "\n",
    "for i, sequence in enumerate(train_data):\n",
    "    print(results[i, sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e7027",
   "metadata": {
    "id": "581e7027"
   },
   "outputs": [],
   "source": [
    "# Now samples look like now:\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab1490",
   "metadata": {
    "id": "b0ab1490"
   },
   "outputs": [],
   "source": [
    "# x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5657770",
   "metadata": {
    "id": "f5657770"
   },
   "outputs": [],
   "source": [
    "# vectorize labels:\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "# Now data is ready to be fed into Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6dcf3",
   "metadata": {
    "id": "90628c00"
   },
   "source": [
    "### Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af935c3",
   "metadata": {
    "id": "3af935c3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb877000",
   "metadata": {
    "id": "cb877000"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu', inpput_shape='10000',))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa360e6",
   "metadata": {
    "id": "3320815c"
   },
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee268a",
   "metadata": {
    "id": "caee268a"
   },
   "outputs": [],
   "source": [
    "# passing optimizer, loss, and metrics as string which is possible bcoz these are packaged as part of Keras\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc8905",
   "metadata": {
    "id": "9d822236"
   },
   "source": [
    "### * configure the parameters of optimizer\n",
    "### * custom loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58809e3",
   "metadata": {
    "id": "c58809e3"
   },
   "outputs": [],
   "source": [
    "# Configuring the optimizer and using custom losses and metrics\n",
    "\n",
    "####### DON'T RUN THIS CELL #########\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizer.RMSprop(lr=0.001, loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e67a5",
   "metadata": {},
   "source": [
    "## Setting aside a validation set\n",
    "\n",
    "* In order to monitor the accuracy of the model during training on the data it has never seen before, we will create a validation set setting apart 10,000 samples from the original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]   # from 0 to 9,999 = 10,000 samples\n",
    "partial_x_train = x_train[10000:]  # from 10,000 to end \n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd713",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=512, \n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b24b16",
   "metadata": {},
   "source": [
    "* On CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds.\n",
    "* At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
    "* call to `model.fit()` returns a History object. This object has the history, which is a dictionary containing data about everything that happened during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd071e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "histroy_dict = history.history\n",
    "\n",
    "history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dict contains four entries\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a0e22",
   "metadata": {},
   "source": [
    "## Plotting the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training Loss') # \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation Loss') # \"b\" is for \"blue solid line\"\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e8722",
   "metadata": {},
   "source": [
    "## Plotting the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f637a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()  # clears the figure\n",
    "\n",
    "accuracy_values = history_dict['accuracy']\n",
    "val_accuracy_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f27ba6",
   "metadata": {},
   "source": [
    "* `Training loss` **decreases** with every epoch\n",
    "* `Training accuracy` **increases** with every epoch\n",
    "* Quantity we are trying to minimize should be `less with every iteration` but that is `not the case for validation loss and accuracy`, they seem to peak at fourth epoch.\n",
    "* To prevent overfitting, we could stop training after three epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d92849",
   "metadata": {},
   "source": [
    "## Retraining a model from scratch\n",
    "Let's train a network from scrach for four epochs and then evaluate it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae063d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = models.Sequential()\n",
    "\n",
    "model1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model1.add(layers.Dense(16, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956529db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(x_train, y_train, epochs=4, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362433d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1.evaluate(x_test, y_test)\n",
    "\n",
    "# The final results are as follows\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fairly naive approach achieves an accuracy of 88%. With state-of-the-art\n",
    "# ..approaches, you should be able to get close to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dfed5",
   "metadata": {},
   "source": [
    "## Generating predictions on the new data using a trained network\n",
    "after we have trained a network, we will want to use it in a practical setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4da50d",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "* You used two hidden layers. Try using `one or three hidden layers`, and see `how doing so affects validation and test accuracy`\n",
    "* Try using layers with `more hidden units` or `fewer hidden units`: `32 units`, `64 units`, and so on.\n",
    "* Try using the `mse loss function` instead of binary_crossentropy.\n",
    "* Try using the `tanh activation` (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67531136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Dense(32, activation='tanh', input_shape=(10000,)))\n",
    "model2.add(layers.Dense(32, activation='tanh'))\n",
    "model2.add(layers.Dense(32, activation='tanh'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model2.evaluate(x_test, y_test)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4291dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e844a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65c001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023dbd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d7a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d5129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103110f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4d33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7f3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdd0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac672eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
